#!/usr/bin/env python3
"""
Write PR Stats to SQLite Database

Reads PR statistics CSV files generated by prStatsAggregator.py and imports
them into the SQLite database pr_stats table. By default, CSV files are deleted
after successful import.

For each user, reads their task files to determine date ranges, then imports
the corresponding stats CSV file: pr_username_start-date_end-date.csv

Usage:
    python prStatsWriteToDB.py [--db-path PATH] [--keep-files]
    
    --keep-files: Keep CSV files after import (default: delete after successful import)
"""

import sqlite3
import csv
import argparse
import os
import json
from pathlib import Path
from typing import List, Dict, Optional
from common import load_all_config, getDBPath


def connect_to_database(db_path: Path, quiet: bool = False) -> Optional[sqlite3.Connection]:
    """
    Connect to the SQLite database.
    
    Args:
        db_path: Path to the database file
        quiet: If True, suppress print statements (default: False)
    
    Returns:
        Database connection or None if failed
    """
    if not db_path.exists():
        if not quiet:
            print(f"âŒ Database not found at: {db_path}")
        return None
    
    if not quiet:
        print(f"ðŸ“ Connecting to database: {db_path}")
    
    try:
        conn = sqlite3.connect(str(db_path))
        return conn
    except Exception as e:
        if not quiet:
            print(f"âŒ Error connecting to database: {e}")
        return None


def parse_bool(value: str) -> int:
    """
    Convert boolean string or value to integer (0 or 1).
    
    Args:
        value: String representation of boolean
    
    Returns:
        1 for True/true/1, 0 for False/false/0 or empty
    """
    if not value or value == '':
        return 0
    
    value_str = str(value).lower().strip()
    
    if value_str in ['true', '1', 'yes']:
        return 1
    elif value_str in ['false', '0', 'no']:
        return 0
    else:
        return 0


def parse_goal_id(value: str) -> Optional[int]:
    """
    Parse goal_id (okr column from CSV).
    
    Args:
        value: String representation of goal ID
    
    Returns:
        Integer goal ID or None if empty/invalid
    """
    if not value or value == '':
        return None
    
    try:
        return int(value)
    except (ValueError, TypeError):
        return None


def parse_confidence(value: str) -> Optional[float]:
    """
    Parse confidence score.
    
    Args:
        value: String representation of confidence
    
    Returns:
        Float confidence or None if empty/invalid
    """
    if not value or value == '':
        return None
    
    try:
        return float(value)
    except (ValueError, TypeError):
        return None


def import_pr_stats_from_csv(conn: sqlite3.Connection, csv_path: Path, quiet: bool = False) -> Dict:
    """
    Import PR stats from a single CSV file into the database.
    
    Args:
        conn: Database connection
        csv_path: Path to CSV file
        quiet: If True, suppress print statements
    
    Returns:
        Dictionary with import results (inserted, updated, errors)
    """
    if not csv_path.exists():
        if not quiet:
            print(f"   âš ï¸  CSV file not found: {csv_path}")
        return {'inserted': 0, 'updated': 0, 'errors': 0}
    
    inserted = 0
    updated = 0
    errors = 0
    
    try:
        with open(csv_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            
            for row in reader:
                try:
                    # Map CSV columns to database columns
                    username = row.get('username', '').strip()
                    pr_id = int(row.get('pr_id', 0))
                    reviewed_authored = row.get('reviewed_authored', '').strip()
                    goal_id = parse_goal_id(row.get('okr', ''))
                    category = row.get('category', '').strip() or None
                    created_time = row.get('created_time', '').strip() or None
                    confidence = parse_confidence(row.get('confidence', ''))
                    author_of_pr = row.get('author_of_pr', '').strip() or None
                    repo = row.get('repo', '').strip() or None
                    is_ai_author = parse_bool(row.get('is-ai-author', ''))
                    
                    # Validate required fields
                    if not username or not pr_id or not reviewed_authored:
                        errors += 1
                        if not quiet:
                            print(f"   âš ï¸  Skipping row with missing required fields: {row}")
                        continue
                    
                    # Check if record exists
                    check_stmt = conn.execute(
                        """
                        SELECT COUNT(*) FROM pr_stats 
                        WHERE username = ? AND pr_id = ? AND reviewed_authored = ?
                        """,
                        (username, pr_id, reviewed_authored)
                    )
                    exists = check_stmt.fetchone()[0] > 0
                    
                    # Insert or replace record
                    insert_stmt = conn.execute(
                        """
                        INSERT OR REPLACE INTO pr_stats 
                        (username, pr_id, reviewed_authored, goal_id, category, created_time, 
                         confidence, author_of_pr, repo, is_ai_author)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                        """,
                        (username, pr_id, reviewed_authored, goal_id, category, created_time,
                         confidence, author_of_pr, repo, is_ai_author)
                    )
                    
                    if exists:
                        updated += 1
                    else:
                        inserted += 1
                
                except Exception as e:
                    errors += 1
                    if not quiet:
                        print(f"   âŒ Error processing row: {e}")
                        print(f"      Row data: {row}")
        
        # Commit all changes
        conn.commit()
        
        if not quiet:
            print(f"   âœ… Processed: {csv_path.name}")
            print(f"      Inserted: {inserted}")
            print(f"      Updated: {updated}")
            print(f"      Errors: {errors}")
    
    except Exception as e:
        if not quiet:
            print(f"   âŒ Error reading CSV file: {e}")
        return {'inserted': 0, 'updated': 0, 'errors': errors + 1}
    
    return {'inserted': inserted, 'updated': updated, 'errors': errors}


def cleanup_user_status_files(username: str, task_folder: Path, quiet: bool = False) -> Dict:
    """
    Clean up intermediate status files and mark main status files as completed.
    
    Files to delete:
    - username_comments_classification_reviewed_status.json
    - username_comments_classification_authored_status.json
    - pr_authored_username_pr-aggregator_status.json
    - pr_reviewed_username_pr-aggregator_status.json
    - pr_authored_okr_username_status.json
    - pr_reviewed_okr_username_status.json
    
    Files to keep but mark as completed:
    - pr_authored_username_status.json
    - pr_reviewed_username_status.json
    
    Args:
        username: Username to clean up files for
        task_folder: Path to task folder
        quiet: If True, suppress print statements
    
    Returns:
        Dictionary with cleanup results
    """
    deleted_count = 0
    updated_count = 0
    errors = []
    
    # Files to delete
    files_to_delete = [
        f"{username}_comments_classification_reviewed_status.json",
        f"{username}_comments_classification_authored_status.json",
        f"pr_authored_{username}_pr-aggregator_status.json",
        f"pr_reviewed_{username}_pr-aggregator_status.json",
        f"pr_authored_okr_{username}_status.json",
        f"pr_reviewed_okr_{username}_status.json",
    ]
    
    # Delete intermediate status files
    for filename in files_to_delete:
        file_path = task_folder / filename
        if file_path.exists():
            try:
                os.remove(file_path)
                deleted_count += 1
                if not quiet:
                    print(f"   ðŸ—‘ï¸  Deleted: {filename}")
            except Exception as e:
                errors.append(f"Failed to delete {filename}: {e}")
                if not quiet:
                    print(f"   âš ï¸  {errors[-1]}")
    
    # Update main status files to mark as completed
    status_files = [
        f"pr_authored_{username}_status.json",
        f"pr_reviewed_{username}_status.json",
    ]
    
    for filename in status_files:
        file_path = task_folder / filename
        if file_path.exists():
            try:
                # Read current status
                with open(file_path, 'r') as f:
                    status_data = json.load(f)
                
                # Update status to completed
                status_data['status'] = 'completed'
                
                # Write back
                with open(file_path, 'w') as f:
                    json.dump(status_data, f, indent=2)
                
                updated_count += 1
                if not quiet:
                    print(f"   âœ… Marked completed: {filename}")
            except Exception as e:
                errors.append(f"Failed to update {filename}: {e}")
                if not quiet:
                    print(f"   âš ï¸  {errors[-1]}")
    
    return {
        'deleted': deleted_count,
        'updated': updated_count,
        'errors': len(errors),
        'error_messages': errors
    }


def import_all_pr_stats(db_path: Path, base_dir: Path, pr_data_folder: Path, task_folder: Path, 
                        users: List[Dict], quiet: bool = False, delete_after_import: bool = True) -> Dict:
    """
    Import PR stats for all users based on their task files.
    
    Args:
        db_path: Path to the database file
        base_dir: Base directory path
        pr_data_folder: PR data folder path
        task_folder: Task folder path
        users: List of user dictionaries
        quiet: If True, suppress print statements
        delete_after_import: If True, delete CSV files after successful import (default: True)
    
    Returns:
        Dictionary with total import results
    """
    stats_folder = pr_data_folder / "pr-stats"
    
    if not stats_folder.exists():
        if not quiet:
            print(f"âŒ Stats folder not found: {stats_folder}")
        return {'total_files': 0, 'inserted': 0, 'updated': 0, 'errors': 0, 'deleted': 0, 
                'cleanup_deleted': 0, 'cleanup_updated': 0}
    
    conn = connect_to_database(db_path, quiet)
    if not conn:
        return {'total_files': 0, 'inserted': 0, 'updated': 0, 'errors': 1, 'deleted': 0,
                'cleanup_deleted': 0, 'cleanup_updated': 0}
    
    try:
        if not quiet:
            print(f"\n{'='*80}")
            print(f"ðŸ“Š Processing {len(users)} user(s)")
            print(f"{'='*80}\n")
        
        total_inserted = 0
        total_updated = 0
        total_errors = 0
        total_deleted = 0
        total_files = 0
        total_cleanup_deleted = 0
        total_cleanup_updated = 0
        
        for user in users:
            username = user.get('userName')
            if not username:
                if not quiet:
                    print(f"âš ï¸  Skipping user with no userName")
                continue
            
            if not quiet:
                print(f"\nðŸ‘¤ User: {username}")
                print(f"{'='*60}")
            
            # Read any task file to get date range (both authored and reviewer should have same dates)
            task_file = None
            for task_type in ["authored", "reviewer"]:
                candidate_file = task_folder / f"pr_{task_type}_{username}.json"
                if candidate_file.exists():
                    task_file = candidate_file
                    break
            
            if not task_file:
                if not quiet:
                    print(f"   âš ï¸  No task files found for user")
                continue
            
            try:
                with open(task_file, 'r') as f:
                    task_data = json.load(f)
                
                start_date = task_data.get('start_date')
                end_date = task_data.get('end_date')
                
                if not start_date or not end_date:
                    if not quiet:
                        print(f"   âš ï¸  Missing date range in {task_file.name}")
                    continue
                
                # Construct CSV filename (no task_type in filename)
                csv_filename = f"pr_{username}_{start_date}_{end_date}.csv"
                csv_path = stats_folder / csv_filename
                
                if not csv_path.exists():
                    if not quiet:
                        print(f"   âš ï¸  Stats file not found: {csv_path}")
                    continue
                
                if not quiet:
                    print(f"   ðŸ“„ Importing: {csv_filename}")
                
                # Import the CSV file
                result = import_pr_stats_from_csv(conn, csv_path, quiet)
                total_inserted += result['inserted']
                total_updated += result['updated']
                total_errors += result['errors']
                total_files += 1
                
                # Only proceed with cleanup and deletion if import was successful
                if result['errors'] == 0:
                    # Delete CSV file after successful import
                    if delete_after_import:
                        try:
                            os.remove(csv_path)
                            total_deleted += 1
                            if not quiet:
                                print(f"   ðŸ—‘ï¸  Deleted: {csv_filename}")
                        except Exception as e:
                            if not quiet:
                                print(f"   âš ï¸  Failed to delete {csv_filename}: {e}")
                    
                    # Clean up intermediate status files and mark main status as completed
                    if not quiet:
                        print(f"   ðŸ§¹ Cleaning up status files...")
                    cleanup_result = cleanup_user_status_files(username, task_folder, quiet)
                    total_cleanup_deleted += cleanup_result['deleted']
                    total_cleanup_updated += cleanup_result['updated']
                    
                    if not quiet and cleanup_result['errors'] > 0:
                        print(f"   âš ï¸  Cleanup had {cleanup_result['errors']} error(s)")
            
            except Exception as e:
                total_errors += 1
                if not quiet:
                    print(f"   âŒ Error processing user {username}: {e}")
        
        if not quiet:
            print(f"\n{'='*80}")
            print(f"âœ… Import Summary")
            print(f"{'='*80}")
            print(f"Total files: {total_files}")
            print(f"Total inserted: {total_inserted}")
            print(f"Total updated: {total_updated}")
            print(f"Total errors: {total_errors}")
            if delete_after_import:
                print(f"Total CSV deleted: {total_deleted}")
            print(f"Cleanup - status files deleted: {total_cleanup_deleted}")
            print(f"Cleanup - status files updated: {total_cleanup_updated}")
            print(f"{'='*80}\n")
        
        return {
            'total_files': total_files,
            'inserted': total_inserted,
            'updated': total_updated,
            'errors': total_errors,
            'deleted': total_deleted,
            'cleanup_deleted': total_cleanup_deleted,
            'cleanup_updated': total_cleanup_updated
        }
    
    finally:
        conn.close()
        if not quiet:
            print("âœ… Database connection closed")


def main():
    """Main function"""
    parser = argparse.ArgumentParser(
        description='Import PR statistics from CSV files into SQLite database'
    )
    parser.add_argument(
        '--db-path',
        type=str,
        help='Path to the SQLite database file (default: from pipeline_config.json)'
    )
    parser.add_argument(
        '--keep-files',
        action='store_true',
        help='Keep CSV files after import (default: delete after successful import)'
    )
    
    args = parser.parse_args()
    
    print("=" * 80)
    print("Write PR Stats to Database")
    print("=" * 80)
    print()
    
    # Load configuration
    all_config = load_all_config()
    config = all_config['config']
    users = all_config['users']
    paths = all_config['paths']
    base_dir = paths['base_dir']
    pr_data_folder = paths['pr_data_folder']
    task_folder = base_dir / config.get('task_folder', 'tasks')
    
    # Determine database path
    if args.db_path:
        db_path = Path(args.db_path)
    else:
        db_path = getDBPath(base_dir)
    
    print(f"ðŸ“ Database: {db_path}")
    print(f"ðŸ“‚ Base directory: {base_dir}")
    print(f"ðŸ“‚ Task folder: {task_folder}")
    print(f"ðŸ“‚ PR data folder: {pr_data_folder}")
    print(f"ðŸ‘¥ Users: {len(users)}")
    print(f"ðŸ—‘ï¸  Delete after import: {not args.keep_files}")
    print()
    
    # Import all PR stats
    delete_after_import = not args.keep_files
    result = import_all_pr_stats(db_path, base_dir, pr_data_folder, task_folder, 
                                 users, quiet=False, delete_after_import=delete_after_import)
    
    # Exit with appropriate code
    if result['errors'] > 0:
        exit(1)
    else:
        exit(0)


if __name__ == "__main__":
    main()
